{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1ac294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nguyen thanh loc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from shared.collator import zero_pad_collator\n",
    "# from shared.tokenizers import HamNoSysTokenizer\n",
    "# from data import get_dataset\n",
    "# from model import IterativeTextGuidedPoseGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86dec40",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12526a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pose_format.torch.masked import MaskedTensor, MaskedTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b7ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tensors(batch: List, pad_value=0):\n",
    "    datum = batch[0]\n",
    "\n",
    "    if isinstance(datum, dict):  # Recurse over dictionaries\n",
    "        return zero_pad_collator(batch)\n",
    "\n",
    "    if isinstance(datum, (int, np.int32)):\n",
    "        return torch.tensor(batch, dtype=torch.long)\n",
    "\n",
    "    if isinstance(datum, (MaskedTensor, torch.Tensor)):\n",
    "        max_len = max(len(t) for t in batch)\n",
    "        if max_len == 1:\n",
    "            return torch.stack(batch)\n",
    "\n",
    "        torch_cls = MaskedTorch if isinstance(datum, MaskedTensor) else torch\n",
    "\n",
    "        new_batch = []\n",
    "        for tensor in batch:\n",
    "            missing = list(tensor.shape)\n",
    "            missing[0] = max_len - tensor.shape[0]\n",
    "\n",
    "            if missing[0] > 0:\n",
    "                padding_tensor = torch.full(missing, fill_value=pad_value, dtype=tensor.dtype, device=tensor.device)\n",
    "                tensor = torch_cls.cat([tensor, padding_tensor], dim=0)\n",
    "\n",
    "            new_batch.append(tensor)\n",
    "\n",
    "        return torch_cls.stack(new_batch, dim=0)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def zero_pad_collator(batch) -> Union[Dict[str, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "    datum = batch[0]\n",
    "\n",
    "    # For strings\n",
    "    if isinstance(datum, str):\n",
    "        return batch\n",
    "\n",
    "    # For tuples\n",
    "    if isinstance(datum, tuple):\n",
    "        return tuple(collate_tensors([b[i] for b in batch]) for i in range(len(datum)))\n",
    "\n",
    "    # For dictionaries\n",
    "    keys = datum.keys()\n",
    "    return {k: collate_tensors([b[k] for b in batch]) for k in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d094f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from fontTools.ttLib import TTFont\n",
    "\n",
    "class BaseTokenizer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokens: List[str],\n",
    "                 starting_index=None,\n",
    "                 init_token=\"[CLS]\",\n",
    "                 eos_token=\"[SEP]\",\n",
    "                 pad_token=\"[PAD]\",\n",
    "                 unk_token=\"[UNK]\"):\n",
    "        if starting_index is None:\n",
    "            starting_index = 4\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "        self.bos_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        self.i2s = {(i + starting_index): c for i, c in enumerate(tokens)}\n",
    "        # Following the same ID scheme as JoeyNMT\n",
    "        self.i2s[0] = self.unk_token\n",
    "        self.i2s[1] = self.pad_token\n",
    "        self.i2s[2] = self.bos_token\n",
    "        self.i2s[3] = self.eos_token\n",
    "        self.s2i = {c: i for i, c in self.i2s.items()}\n",
    "\n",
    "        self.pad_token_id = self.s2i[self.pad_token]\n",
    "        self.bos_token_id = self.s2i[self.bos_token]\n",
    "        self.eos_token_id = self.s2i[self.eos_token]\n",
    "        self.unk_token_id = self.s2i[self.unk_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.i2s)\n",
    "\n",
    "    def vocab(self):\n",
    "        return list(self.i2s.values())\n",
    "\n",
    "    def text_to_tokens(self, text: str) -> List[str]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tokens_to_text(self, tokens: List[str]) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tokenize(self, text: str, bos=False, eos=False):\n",
    "        tokens = [self.s2i[c] for c in self.text_to_tokens(text)]\n",
    "        if bos:\n",
    "            tokens.insert(0, self.bos_token_id)\n",
    "        if eos:\n",
    "            tokens.append(self.eos_token_id)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def detokenize(self, tokens: List[int]):\n",
    "        if len(tokens) == 0:\n",
    "            return \"\"\n",
    "        if tokens[0] == self.bos_token_id:\n",
    "            tokens = tokens[1:]\n",
    "        if tokens[-1] == self.eos_token_id:\n",
    "            tokens = tokens[:-1]\n",
    "\n",
    "        try:\n",
    "            padding_index = tokens.index(self.pad_token_id)\n",
    "            tokens = tokens[:padding_index]\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        return self.tokens_to_text([self.i2s[t] for t in tokens])\n",
    "\n",
    "    def __call__(self, texts, is_tokenized=False, device=None):\n",
    "        if not is_tokenized:\n",
    "            all_tokens = [self.tokenize(text) for text in texts]\n",
    "        else:\n",
    "            all_tokens = texts.tolist()\n",
    "\n",
    "        tokens_batch = zero_pad_collator([{\n",
    "            \"tokens_ids\": torch.tensor(tokens, dtype=torch.long, device=device),\n",
    "            \"attention_mask\": torch.ones(len(tokens), dtype=torch.bool, device=device),\n",
    "            \"positions\": torch.arange(0, len(tokens), dtype=torch.int, device=device)\n",
    "        } for tokens in all_tokens])\n",
    "        # In transformers, 1 is mask, not 0\n",
    "        tokens_batch[\"attention_mask\"] = torch.logical_not(tokens_batch[\"attention_mask\"])\n",
    "\n",
    "        return tokens_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382f85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HamNoSysTokenizer(BaseTokenizer):\n",
    "\n",
    "    def __init__(self, starting_index=None, **kwargs):\n",
    "        self.font_path = \"./shared/tokenizers/hamnosys/HamNoSysUnicode.ttf\"\n",
    "\n",
    "        with TTFont(self.font_path) as font:\n",
    "            tokens = [chr(key) for key in font[\"cmap\"].getBestCmap().keys()]\n",
    "\n",
    "        super().__init__(tokens=tokens, starting_index=starting_index, **kwargs)\n",
    "\n",
    "    def text_to_tokens(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def tokens_to_text(self, tokens: List[str]) -> str:\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d85da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_to_pose.data import get_dataset\n",
    "# from text_to_pose.model import IterativeTextGuidedPoseGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56927277",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799a93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_format import Pose\n",
    "from torch.utils.data import Dataset\n",
    "from shared.tfds_dataset import ProcessedPoseDatum, get_tfds_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb896d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPoseDatum(TypedDict):\n",
    "    id: str\n",
    "    text: str\n",
    "    pose: Pose\n",
    "    length: int\n",
    "\n",
    "\n",
    "class TextPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data: List[TextPoseDatum]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        pose = datum[\"pose\"]\n",
    "\n",
    "        torch_body = pose.body.torch()\n",
    "        pose_length = len(torch_body.data)\n",
    "\n",
    "        return {\n",
    "            \"id\": datum[\"id\"],\n",
    "            \"text\": datum[\"text\"],\n",
    "            \"pose\": {\n",
    "                \"obj\": pose,\n",
    "                \"data\": torch_body.data.tensor[:, 0, :, :],\n",
    "                \"confidence\": torch_body.confidence[:, 0, :],\n",
    "                \"length\": torch.tensor([pose_length], dtype=torch.float),\n",
    "                \"inverse_mask\": torch.ones(pose_length, dtype=torch.int8)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def process_datum(datum: ProcessedPoseDatum) -> TextPoseDatum:\n",
    "    text = datum[\"tf_datum\"][\"hamnosys\"].numpy().decode('utf-8').strip()\n",
    "    pose: Pose = datum[\"pose\"]\n",
    "\n",
    "    # Prune all leading frames containing only zeros\n",
    "    for i in range(len(pose.body.data)):\n",
    "        if pose.body.confidence[i].sum() != 0:\n",
    "            if i != 0:\n",
    "                pose.body.data = pose.body.data[i:]\n",
    "                pose.body.confidence = pose.body.confidence[i:]\n",
    "            break\n",
    "\n",
    "    return {\"id\": datum[\"id\"], \"text\": text, \"pose\": pose, \"length\": max(len(pose.body.data), len(text))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103c9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name=\"dicta_sign\",\n",
    "                poses=\"holistic\",\n",
    "                fps=25,\n",
    "                split=\"train\",\n",
    "                components: List[str] = None,\n",
    "                data_dir=None,\n",
    "                max_seq_size=1000):\n",
    "    data = get_tfds_dataset(name=name, poses=poses, fps=fps, split=split, components=components, data_dir=data_dir)\n",
    "\n",
    "    data = [process_datum(d) for d in data]\n",
    "    data = [d for d in data if d[\"length\"] < max_seq_size]\n",
    "\n",
    "    return TextPoseDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f685267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e10187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.models.pose_encoder import PoseEncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f745ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(loss_type, pose: torch.Tensor, pose_hat: torch.Tensor, confidence: torch.Tensor):\n",
    "    # Loss by confidence. If missing joint, no loss. If less likely joint, less gradients.\n",
    "    if loss_type == 'l1':\n",
    "        error = torch.abs(pose - pose_hat).sum(-1)\n",
    "    elif loss_type == 'l2':\n",
    "        error = torch.pow(pose - pose_hat, 2).sum(-1)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    return (error * confidence).mean()\n",
    "\n",
    "\n",
    "class DistributionPredictionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_mu = nn.Linear(input_size, 1)\n",
    "        self.fc_var = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mu = self.fc_mu(x)\n",
    "        if not self.training:  # In test time, just predict the mean\n",
    "            return mu\n",
    "\n",
    "        log_var = self.fc_var(x)\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        return q.rsample()\n",
    "\n",
    "\n",
    "class IterativeTextGuidedPoseGenerationModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 pose_dims: (int, int) = (137, 2),\n",
    "                 hidden_dim: int = 128,\n",
    "                 text_encoder_depth=2,\n",
    "                 pose_encoder_depth=4,\n",
    "                 encoder_heads=2,\n",
    "                 encoder_dim_feedforward=2048,\n",
    "                 max_seq_size: int = 1000,\n",
    "                 loss_type='l1'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_size = max_seq_size\n",
    "\n",
    "        # Embedding layers\n",
    "        self.positional_embeddings = nn.Embedding(num_embeddings=max_seq_size, embedding_dim=hidden_dim)\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(tokenizer),\n",
    "            embedding_dim=hidden_dim,\n",
    "            padding_idx=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        self.pose_encoder = PoseEncoderModel(pose_dims=pose_dims,\n",
    "                                             hidden_dim=hidden_dim,\n",
    "                                             encoder_depth=pose_encoder_depth,\n",
    "                                             encoder_heads=encoder_heads,\n",
    "                                             encoder_dim_feedforward=encoder_dim_feedforward,\n",
    "                                             max_seq_size=max_seq_size,\n",
    "                                             dropout=0)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim,\n",
    "                                                   nhead=encoder_heads,\n",
    "                                                   dim_feedforward=encoder_dim_feedforward,\n",
    "                                                   batch_first=True)\n",
    "        self.text_encoder = nn.TransformerEncoder(encoder_layer, num_layers=text_encoder_depth)\n",
    "\n",
    "        # Predict sequence length\n",
    "        self.seq_length = DistributionPredictionModel(hidden_dim)\n",
    "\n",
    "        # Predict pose difference\n",
    "        self.pose_diff_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.pose_encoder.pose_dim),\n",
    "        )\n",
    "\n",
    "        # Loss\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def encode_text(self, texts: List[str]):\n",
    "        tokenized = self.tokenizer(texts, device=self.device)\n",
    "        positional_embedding = self.positional_embeddings(tokenized[\"positions\"])\n",
    "        embedding = self.embedding(tokenized[\"tokens_ids\"]) + positional_embedding\n",
    "        encoded = self.text_encoder(embedding, src_key_padding_mask=tokenized[\"attention_mask\"])\n",
    "        seq_length = self.seq_length(torch.mean(encoded, dim=1))\n",
    "        return {\"data\": encoded, \"mask\": tokenized[\"attention_mask\"]}, seq_length\n",
    "\n",
    "    def refine_pose_sequence(self, pose_sequence, text_encoding):\n",
    "        batch_size, seq_length, _, _ = pose_sequence[\"data\"].shape\n",
    "        pose_encoding = self.pose_encoder(pose=pose_sequence, additional_sequence=text_encoding)\n",
    "        pose_encoding = pose_encoding[:, :seq_length, :]\n",
    "\n",
    "        # Predict desired change\n",
    "        flat_pose_projection = self.pose_diff_projection(pose_encoding)\n",
    "        return flat_pose_projection.reshape(batch_size, seq_length, *self.pose_encoder.pose_dims)\n",
    "\n",
    "    def forward(self, text: str, first_pose: torch.Tensor, step_size: float = 0.5):\n",
    "        text_encoding, sequence_length = self.encode_text([text])\n",
    "        sequence_length = round(float(sequence_length))\n",
    "\n",
    "        pose_sequence = {\n",
    "            \"data\": first_pose.expand(1, sequence_length, *self.pose_encoder.pose_dims),\n",
    "            \"mask\": torch.zeros([1, sequence_length], dtype=torch.bool),\n",
    "        }\n",
    "        while True:\n",
    "            yield pose_sequence[\"data\"][0]\n",
    "\n",
    "            step = self.refine_pose_sequence(pose_sequence, text_encoding)\n",
    "            pose_sequence[\"data\"] = pose_sequence[\"data\"] + step_size * step\n",
    "\n",
    "    def training_step(self, batch, *unused_args, steps=100):\n",
    "        return self.step(batch, *unused_args, steps=steps, name=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, *unused_args, steps=100):\n",
    "        return self.step(batch, *unused_args, steps=steps, name=\"validation\")\n",
    "\n",
    "    def step(self, batch, *unused_args, steps: int, name: str):\n",
    "        text_encoding, sequence_length = self.encode_text(batch[\"text\"])\n",
    "        pose = batch[\"pose\"]\n",
    "\n",
    "        # Calculate sequence length loss\n",
    "        sequence_length_loss = F.mse_loss(sequence_length, pose[\"length\"]) / 10000\n",
    "\n",
    "        # Repeat the first frame for initial prediction\n",
    "        batch_size, pose_seq_length, _, _ = pose[\"data\"].shape\n",
    "        pose_sequence = {\n",
    "            \"data\": torch.stack([pose[\"data\"][:, 0]] * pose_seq_length, dim=1),\n",
    "            \"mask\": torch.logical_not(pose[\"inverse_mask\"])\n",
    "        }\n",
    "\n",
    "        refinement_loss = 0\n",
    "        for _ in range(steps):\n",
    "            pose_sequence[\"data\"] = pose_sequence[\"data\"].detach()  # Detach from graph\n",
    "            l1_gold = pose[\"data\"] - pose_sequence[\"data\"]\n",
    "            l1_predicted = self.refine_pose_sequence(pose_sequence, text_encoding)\n",
    "            refinement_loss += masked_loss(self.loss_type, l1_gold, l1_predicted, confidence=pose[\"confidence\"])\n",
    "\n",
    "            step_size = 1 / steps\n",
    "            l1_step = l1_gold if name == \"validation\" else l1_predicted\n",
    "            pose_sequence[\"data\"] = pose_sequence[\"data\"] + step_size * l1_step\n",
    "\n",
    "            if name == \"train\":  # add just a little noise while training\n",
    "                pose_sequence[\"data\"] = pose_sequence[\"data\"] + torch.randn_like(pose_sequence[\"data\"]) * 1e-4\n",
    "\n",
    "        self.log(name + \"_seq_length_loss\", sequence_length_loss, batch_size=batch_size)\n",
    "        self.log(name + \"_refinement_loss\", refinement_loss, batch_size=batch_size)\n",
    "        loss = refinement_loss + sequence_length_loss\n",
    "        self.log(name + \"_loss\", loss, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588dbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dfbcc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Nguyen Thanh Loc/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20221107_111134-r3juckee</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/maximusss98/text-to-pose/runs/r3juckee\" target=\"_blank\">zesty-grass-1</a></strong> to <a href=\"https://wandb.ai/maximusss98/text-to-pose\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOGGER = WandbLogger(project=\"text-to-pose\", log_model=False, offline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(poses=args.pose,\n",
    "                                fps=args.fps,\n",
    "                                components=args.pose_components,\n",
    "                                max_seq_size=args.max_seq_size,\n",
    "                                split=\"train[10:]\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=zero_pad_collator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
